\documentclass[a4paper,11pt]{article}

%---enable russian----

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
%\usepackage[T2A]{fontenc}


\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amscd,amsmath,amsthm,euscript}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode} 



% PROBABILITY SYMBOLS
\newcommand*\PROB\Pr 
\DeclareMathOperator*{\EXPECT}{\mathbb{E}}


% Sets, Rngs, ets 
\newcommand{\N}{{{\mathbb N}}}
\newcommand{\Z}{{{\mathbb Z}}}
\newcommand{\R}{{{\mathbb R}}}
\newcommand{\Zp}{\ints_p} % Integers modulo p
\newcommand{\Zq}{\ints_q} % Integers modulo q
\newcommand{\Zn}{\ints_N} % Integers modulo N

% Landau 
\newcommand{\bigO}{\mathcal{O}}
\newcommand*{\OLandau}{\bigO}
\newcommand*{\WLandau}{\Omega}
\newcommand*{\xOLandau}{\widetilde{\OLandau}}
\newcommand*{\xWLandau}{\widetilde{\WLandau}}
\newcommand*{\TLandau}{\Theta}
\newcommand*{\xTLandau}{\widetilde{\TLandau}}
\newcommand{\smallo}{o} %technically, an omicron
\newcommand{\softO}{\widetilde{\bigO}}
\newcommand{\wLandau}{\omega}
\newcommand{\negl}{\mathrm{negl}} 
\newcommand*{\poly}{\ensuremath{\mathrm{poly}}}

% Misc
\newcommand{\eps}{\varepsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}


\newtheorem{theorem}{Теорема}
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{observation}[theorem]{Замечание}
\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{definition}[theorem]{Определение}
\newtheorem{claim}[theorem]{Утверждение}
\newtheorem{fact}[theorem]{Факт}
\newtheorem{assumption}[theorem]{Предположение}

% 1-inch margins
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex



\begin{document}

\section{Алгоритм Coded-BKW with Sieving}

	
	
	\begin{algorithm}[ph]
		\caption{Coded-BKW with Sieving}
		\label{alg:AlgName}
		\textbf{Вход:}  Матрица $A$ размерности $n \times m$, \\
		Полученный вектор $z$ длины $m$, \\
		$t$ - количество шагов,\\
		$n_i$ - длина линейного кода $[n_i,b]$,\\
		$1 \leq i < t$ - индексы,\\
		$B$ - константа\\
		
		\begin{algorithmic}[1]
			
			\State{Изменяется распределение секретного вектора (по методу Гаусса).}
			\For{$i$ from 1 to $t$}
				\For{$h \in H_{i-1}$ from 1 to $t$}
					\State $\Delta=CodeMap(h,i)$
					\State $h$ записывается в список $L_{\Delta}$
				\EndFor
				\For{всех списков $L_{\Delta}$}
					\State $S_{\Delta} = Sieve(L_{\Delta}, i, \sqrt{N_i}\cdot B)$
					\State Все $S_{\Delta}$ записываются в качестве столбцов в $H_i$
				\EndFor
			\EndFor
			\State Происходит угадывание $s_n$ при помощи проверки гипотез.
		\end{algorithmic}
		
	\end{algorithm}
	
\textbf{Замечания к алгоритму:}\\
\begin{enumerate}
\item Полученный на вход вектор $z$ имеет вид $(s,e)\begin{pmatrix} A\\ I \end{pmatrix} = z \Rightarrow (s,e)H_0 = z$, где $H_0 = \begin{pmatrix} A\\ I \end{pmatrix}$, $s$ - секретный вектор в $Z^{n}_q$, $e$ - вектор ошибок.
\item Константа $B$ обозначает средний уровень «мелкости» для позиции.
\item Опишем процедуру $CodeMap$. В соответствии с основной идеей Coded-BKW, фиксируем код $C_i$ длины $n_i$. Вектор $h$, полученный на входе, сначала считается ограниченным только позициями $N_i+1$ до $N_{i-1}$, т.е. вектор длины $n_i$. Этот вектор, обозначенный $h_{[N_{i-1}+1,N_i]}$, затем отображается в ближайшее кодовое слово в $C_i$. Это ближайшее кодовое слово и обозначается $CodeMap(h,i)$.
\item Опишем процедуру $Sieve$. Полученное на входе $L_{\Delta}$ содержит список векторов. Мы рассматриваем их только первые $N_i$ позиций. Эта процедура найдет разности между любыми двумя векторами такими, что норма разности, ограниченная первыми $N_i$ позициями, меньше, чем $\sqrt{N_i}\cdot B$. Все такие разности отправляются в список $S_{\Delta}$, что и есть результат процедуры. Обычно, список $S_{\Delta}$ содержит примерно столько же векторов, сколько и $L_{\Delta}$. Предполагаем, что векторы в $L_{\Delta}$ ограниченные первыми $N_i$ позициями все имеют норму примерно равную $\sqrt{N_i}\cdot B$. Тогда задача решается алгоритмами просеивания в решетках, например, используя LSH.
\end{enumerate}

\textbf{Пример первого шага алгоритма:}\\
Столбец $h \in H_0$ сначала попадает в процедуру $\Delta=CodeMap(h,1)$. Затем мы помещаем $h$ в список $L_{\Delta}$. После пробега по всем столбцам $h \in H_0$ они оказываются рассортированными в списки $L_{\Delta}$. Обозначим за $K$ общее число этих списков. Затем пробегаем по всем спискам, каждый содержит примерно $m/K$ столбцов. Производим шаг просеивания  $S_{\Delta} = Sieve(L_{\Delta}, \sqrt{N_1}\cdot B)$, для всех $\Delta \in C_i$. Результатом является список векторов, где норма каждого вектора, ограниченная первыми $N_1$ позициями не превышает $\sqrt{N_1}\cdot B$. Все векторы во всех списках $S_{\Delta}$ теперь помещаются в $H_1$ как столбцы. Теперь мы имеем матрицу $H_1$, где норма каждого столбца, ограниченная первыми $n_1$ позициями, не превышает $\sqrt{N_1}\cdot B$. Конец.

\section{Анализ алгоритма}

Сразу хотел бы сказать, что с пониманием анализа у меня большие проблемы, ибо сам я никогда этим не занимался и даже некоторые обозначения у меня вызывают вопросы. Поэтому отчеты по этой части будут состоять из вопросов, как может показаться, по очевидным вещам.

\textbf{1. Хотелось бы на всякий случай уточнить, что понимается под «магнитудой» (magnitude). Это параметр $B$ } \footnote{E: 
Под ``магнитудой', как правило, понимается норма (Евклидова или $\ell_\infty$). По ходу алгоритма наша задача -- производить вектора  Евклидовой нормы меньше $\leq \sqrt{n_i}B$, следовательно, в конце алгоритма получить вектора Евклидовой нормы $ \leq \sqrt{n}B$. Полагая, что координаты этих векторов примерно равны, мы имеем, что каждая координата полученного вектора $\leqq B$. Заметьте, что этот параметр - свободный и выбирается во время анализа. В итоге, он равен $\poly(n)$
} 

\textbf{2. Разберем самое начало пункта VI PARAMETER SELECTION AND ASYMPTOTIC ANALYSIS до подпункта A.}

После каждого шага, позиции, который уже были обработаны, должны оставаться на некоторой заданной магнитуде $B$. Т.е. среднее (абсолютное значение) обработанной позиции должно быть очень близким к $B$. Это обусловлено тем, что мы применяем просеивание на каждом шаге редукции. После $t$ шагов мы получаем векторы средней нормы $\sqrt{n}\cdot B$.\footnote{E: именно так} 

Зададим число выборок равным ${m=2^k}$, где $2^k$ это параметр, который будет определять общую сложность алгоритма \textcolor{red}{(можете немного пояснить?)}\footnote{E: в этом применяется следующий подход: 1. сложность алгоритма, очевидно, зависит от количества необходимый выборок. Более того, если для верной работы алгоритмы нам нужно $m$ выборок, то время работы всего алгоритма будет напрямую зависеть от $m$. Конкретнее, время работы будет как минимум $\bigO(m)$, так как нам нужно все элементы выборки как минимум записать в список. 2. Мы ``знаем'', что размер необходимой выборки  будет экспоненциальным от $n$, т.е.\ $m = 2^{c \cdot n + \smallo(n) }$, где $c$ - некая константа (``знаем'' мы с учетом анализа предыдущих алгоритмов BKW.) Вопрос в том, чему же равна эта константа (в обозначения статьи $k=c_0 \cdot n$). Если она будет меньше, чем для предыдущих алгоритмов, то скорее всего, новый алгоритм работает быстрее. Что и происходит в нашем случае.}
мы получим примерно $m=2^k$ выборок после $t$ шагов. Как было описано ранее в статье, полученные выборки будут примерно Гауссовыми с дисперсией $\sigma^2\cdot(nB^2+2^t)$. Мы предполагаем, что лучшая стратегия это сохранение магнитуд двух различных \textcolor{red}{contributions (не очень понял, как корректно перевести)}\footnote{E: здесь имеется ввиду двух слагаемых $nB^2$ и $2^t$. Т.е.\ оба слагаемых ``добавляют'' (contribute) одинаковое количество шума в финальную выборку} одного и того же порядка, т.о. мы выбираем $nB^2 \approx 2^t$.

Далее, используя равенство $C \cdot e^{2\pi}(\frac{\sigma \sqrt{2\pi}}{q})^2$, \footnote{E: поясните, что такое $C$, откуда взялось это выражение, и что оно обозначает} чтобы иметь возможность восстановить одну секретную позицию, используя $m$ выборок, нам необходимо $m=\bigO\left(e^{4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2}}\right)$.\footnote{E: вам понятно, откуда взялось это необходимое условие? Если нет, обратитесь к секции 4.2 Hypothesis testing в \url{https://eprint.iacr.org/2015/056.pdf}. Еще я про это рассказывала на прошлой неделе, в моем черновике доклада на первой странице я говорю про bias (``уклон'') полученного в результате Гауссового распределения }

Т.о., мы имеем $\ln{2} \cdot k = 4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2} + \bigO(1)$.

\textcolor{red}{Как уже сказал ранее, я вообще не разбираюсь в символах O и остальных. Можете в качестве примера и объяснения разобрать этот и предыдущий ($m$ и $\ln{2} \cdot k$) результат?}\footnote{E: здесь примерами не обойтись. Надо прочесть \url{http://web.mit.edu/16.070/www/lecture/big_o.pdf}
Для понимания анализа любого алгоритма, даже сортировки пузырьком, символы Ландау необходимы
}
\footnote{Второе выражение получено из первого взятием натурального логарифма и а также замечанием, что $\bigO(\exp(n)) = c \cdot \exp(X) + \smallo(\exp(X))$ для некой константы $c$, следовательно, $ \ln(\bigO(\exp(n)) ) = \ln(c) + X = \bigO(1) + X$ ($\bigO(1)$ -- обозначение константы $\ln(c)$).  }

Каждый из $t$ шагов должен доставить $m=2^k$ векторов вида описанного ранее.\footnote{E: Это не совсем верно, хоть так и написано в статье. На каждом шаге мы теряем $\bigO(2^k)$ векторов. Но так как шагов у нас будет всего $t = \bigO(\log n)$ (более конкретное значение дано в уравнении $(8)$ статьи), мы можем считать, что на выходе мы будем иметь $2^k$ векторов, при этом увеличив количество векторов на входе до $\approx \log n \cdot 2^k$. На асимптотику это фактор $\log(n)$ не влияет}

Поскольку мы имеем две части на каждом шаге редукции, нам необходимо проанализировать эти части по отдельности. Во-первых, рассмотрим выполнение первой части $i$ - го шага редукции, используя Coded-BKW с $[n_i,d_i]$ линейным кодом, где параметры $n_i$ и $d_i$ на каждом шаге выбраны для оптимального (глобально) выполнения. Мы сортируем $2^k$ векторов по $K=\frac{q^{d_i - 1}}{2}$ различным спискам. Здесь Coded-BKW шаг гарантирует, что все векторы в списке, ограниченные $n_i$ рассматриваемыми позициями, имеют среднюю норму не более чем $\sqrt{n_i}\cdot B$, если кодовое слово вычитается из вектора. Т.о. количество списков $\frac{q^{d_i - 1}}{2}$ должно быть выбрано так, чтобы это ограничение нормы выполнялось.\footnote{E: не совсем понимаю смысл этого предложения (в англ.\ версии тоже). Поясните} Затем, после шага Coded-BKW, шаг просеивания должен оставить среднюю норму по $N_i$ позициям без изменений, т.е. не более $\sqrt{N_i}\cdot B$.

Поскольку все векторы в списке, можно считать, имеют норму $\sqrt{N_i}\cdot B$ в этих $N_i$ позициях, шагу просеивания необходимо найти любую пару, которая оставляет разницу между нормами двух векторов не более $\sqrt{N_i}\cdot B$. Используя эвристику, что векторы равномерно распределены на сфере радиуса $\sqrt{N_i}\cdot B$, мы знаем, что один список должен содержать не менее $2^{0.208N_i }$ векторов, чтобы иметь возможность производить такое же количество векторов. Оценка времени и пространства\footnote{E: это только касается времени. Память ограничена $2^{0.208n }$. Авторы - не эксперты в LSH} равна $2^{0.292N_i}$, если используется LSF.

Примем некоторые дальнейшие обозначения. Поскольку мы ожидаем, что число векторов будет экспоненциально, мы пишем $k=c_0n$ для некоторого $c_0$.\footnote{E: это как раз отвечает на ваш вопрос страницей выше} Также, мы принимаем\footnote{E: скорее ``полагаем''} $q=n^{c_q}$ и $\sigma=n^{c_s}$. Выбрав $nB^2\approx2^t$, из формулы $\ln{2} \cdot k = 4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2} + O(1)$ выводим, что \textcolor{red}{$B=\Theta(n^{c_q-c_s})$} и \textcolor{red}{$t=(2(c_q-c_s)+1)\log_2 n + O(1)$}. \footnote{E: выражение для $t$ получено из выражения для $B$ и $nB^2 = 2^t$, думаю, это понятно. А выражение для $B$ получено из равенства выше, игнорируя константы (они спрятаны в обозначении $\Theta$)}

\textbf{Рассмотрим пункт A. Asymptotics of Coded-BKW with sieving.}\\

Предположим, что общая сложность экспоненциальна и запишем ее как $2^{cn}$, для некоторого коэффициента $c$, который определим позднее. Каждый шаг является аддитивным с точки зрения сложности \textcolor{red}{(не очень понял этот момент)}\footnote{E: Для этого надо описать алгоритм и понять, что есть ``шаг''. Сейчас это не определено, не понятно, что мы анализируем}, поэтому предположим, что мы можем использовать $2^{cn}$ на каждом шаге. В $t$ шагах мы выбираем $n_1,n_2,…$ позиций для каждого шага  \textcolor{red}{(примерно понял, но наверное некорректно перевожу). \footnote{E: Позиций для чего? Надо уточнить. Опять же, будет уместным описать подробно алгоритм}}

Число \textcolor{red}{buckets (не знаю, как корректно перевести)} \footnote{E: прямой перевод ``корзин'' здесь подойдет. Речь идет об элементах, совпавших при декодировании на $i$-ом шаге (т.е. близких друг другу на координатах $n_i$). Эти вектора попадают в одну ``корзину''. Более точно, в одну хэш-таблицу, если думать о декодировании как о функции хэширования} необходимых для первого шага coded-BKW равно $(C^{'} \cdot n^{c_s})^{n_1}$, где $C^{'}$ другая константа. В каждом \textcolor{red}{bucket} основную часть сложности по времени составляет просеивание стоимостью $2^{\lambda n_1}$, для константы $\lambda$. Итоговая сложность, есть произведение этих выражений, которое должно соответствовать границе $2^{cn}$, и таким образом, мы выбираем $n_1$ таким, что $(C^{'} \cdot n^{c_s})^{n_1} \approx 2^{cn} \cdot 2^{- \lambda n_1}$ \textcolor{red}{(если можно краткое пояснение по итоговой формуле)}.\footnote{E: Мне тоже непонятно, что вы здесь имеете в виду. Например, откуда взялась сложность $(C^{'} \cdot n^{c_s})^{n_1}$? Сперва надо разобраться с каждым из множителей.}

Прологарифмируем, $c_s \log{n} \cdot n_1 + \log{C^{'}} n_1 = cn - \lambda n_1$. Таким образом, получаем $n_1 = \frac{cn}{c_s \log{n} + \lambda + \log{C^{'}}}$. Чтобы упростить выражение, введем обозначение $W=c_s \log{n} + \lambda + \log{C^{'}}$.\footnote{E: зачем носить за собой эту константу $C'$?}

Для следующего шага, мы получаем $W \cdot n_2 = cn - \lambda n_1$,\footnote{E: поясните, почему} что упрощается с асимптотической точки зрения до $n_2 = \frac{cn}{W} (1 - \frac{\lambda}{W})$.

Продолжая по такой логике, мы имеем, что $W \cdot n_i = cn - \lambda \sum_{j=1}^{i-1}  n_j$ и мы можем получить асимптотическое выражение для $n_i$: $n_i = \frac{cn}{W} (1 - \frac{\lambda}{W})^{i-1}$.

После $t$ шагов мы имеем $\sum_{i=1}^t n_i = n$\footnote{E: это необходимое условие для покрытия всех координат, а не результат алгоритма.} таким образом, мы замечаем, что $\sum_{i=1}^t n_i = \frac{cn}{W} \sum_{i=1}^t (1 - \frac{\lambda}{W})^{i-1}$, что упрощается до $n = \sum_{i=1}^t n_i = \frac{cn}{\lambda} (1-(1-\frac{\lambda}{W})^t)$.

Теперь мы знаем, что $c=\lambda (1-(1-\frac{\lambda}{W})^t)^{-1}$.

Поскольку $t$ и $W$ оба порядка $\Theta(\log{n})$  \textcolor{red}{(требуется небольшое пояснение)},\footnote{E: формулу для $t$ вы получили в предыдущем отчете, для $W$ достаточно посмотреть на определение } которое стремится к бесконечности как $n$ стремится к бесконечности,\footnote{E: Не понимаю, к чему этот комментарий о пределе} мы имеем, что $c=\lambda (1-(1-\frac{\lambda}{W})^{\frac{W}{\lambda} \cdot \frac{t \lambda}{W}})^{-1} \rightarrow \lambda (1-e^{- \frac{t \lambda}{W}})^{-1}$, когда $n \rightarrow \infty$.

Поскольку $t/W \rightarrow (1+2(c_q + c_s))/c_s$, когда $n \rightarrow \infty$ в итоге мы получаем $c=\frac{\lambda}{1-e^{- \lambda(1+2(c_q + c_s))/c_s}}$.

Теперь предположим просеивающую эвристику,\footnote{E: не совсем верный (хотя и дословный) перевод. Уберите слово ``просеивающую''} что после $i$ - го шага, векторы, ограниченные первыми $N_i$ позициями, распределены равномерно на сфере радиуса $\sqrt{N_i}\cdot B$. Также предположим, что дискретное Гауссово распределение может быть аппроксимировано непрерывным. Тогда мы получаем следующую теорему.

\textbf{Теорема 1.}\\
Сложность по времени и пространству предложенного алгоритма равна $2^{(c+o(1))^n}$, где $c=\frac{\lambda}{1-e^{- \lambda(1+2(c_q + c_s))/c_s}}$, и $\lambda = 0,292$ для обычных компьютеров и $0,265$ для квантовых.

\textbf{Доказательство:}\\
Поскольку $c> \lambda$, для процесса различения остается экспоненциальное число выборок. Можно скорректировать константы в формулах $B=\theta(n^{c_q-c_s})$ и $t=(2(c_q-c_s)+1)\log_2 n + O(1)$, чтобы вероятность успеха гипотезы была близка к 1.

\textbf{Итог: для понимания этого разбора мне необходимо понять самое начало, так сказать отправные точки.\footnote{E: Думаю, вам стоит для понимания описать сначала сам алгоритм} С выведением более-менее все понятно. Хотя еще можно было бы чуть разобрать формулу $2^{(c+o(1))^n}$ для полного понимания.}\\

\end{document}