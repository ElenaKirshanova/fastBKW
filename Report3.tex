\documentclass[a4paper,11pt]{article}

%---enable russian----

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
%\usepackage[T2A]{fontenc}


\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amscd,amsmath,amsthm,euscript}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode} 



% PROBABILITY SYMBOLS
\newcommand*\PROB\Pr 
\DeclareMathOperator*{\EXPECT}{\mathbb{E}}


% Sets, Rngs, ets 
\newcommand{\N}{{{\mathbb N}}}
\newcommand{\Z}{{{\mathbb Z}}}
\newcommand{\R}{{{\mathbb R}}}
\newcommand{\Zp}{\ints_p} % Integers modulo p
\newcommand{\Zq}{\ints_q} % Integers modulo q
\newcommand{\Zn}{\ints_N} % Integers modulo N

% Landau 
\newcommand{\bigO}{\mathcal{O}}
\newcommand*{\OLandau}{\bigO}
\newcommand*{\WLandau}{\Omega}
\newcommand*{\xOLandau}{\widetilde{\OLandau}}
\newcommand*{\xWLandau}{\widetilde{\WLandau}}
\newcommand*{\TLandau}{\Theta}
\newcommand*{\xTLandau}{\widetilde{\TLandau}}
\newcommand{\smallo}{o} %technically, an omicron
\newcommand{\softO}{\widetilde{\bigO}}
\newcommand{\wLandau}{\omega}
\newcommand{\negl}{\mathrm{negl}} 
\newcommand*{\poly}{\ensuremath{\mathrm{poly}}}

% Misc
\newcommand{\eps}{\varepsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}


\newtheorem{theorem}{Теорема}
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{observation}[theorem]{Замечание}
\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{definition}[theorem]{Определение}
\newtheorem{claim}[theorem]{Утверждение}
\newtheorem{fact}[theorem]{Факт}
\newtheorem{assumption}[theorem]{Предположение}

% 1-inch margins
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex



\begin{document}
	


Сразу хотел бы сказать, что с пониманием анализа у меня большие проблемы, ибо сам я никогда этим не занимался и даже некоторые обозначения у меня вызывают вопросы. Поэтому отчеты по этой части будут состоять из вопросов, как может показаться, по очевидным вещам.

\textbf{1. Хотелось бы на всякий случай уточнить, что понимается под «магнитудой» (magnitude). Это параметр $B$ } \footnote{E: 
Под ``магнитудой', как правило, понимается норма (Евклидова или $\ell_\infty$). По ходу алгоритма наша задача -- производить вектора  Евклидовой нормы меньше $\leq \sqrt{n_i}B$, следовательно, в конце алгоритма получить вектора Евклидовой нормы $ \leq \sqrt{n}B$. Полагая, что координаты этих векторов примерно равны, мы имеем, что каждая координата полученного вектора $\leqq B$. Заметьте, что этот параметр - свободный и выбирается во время анализа. В итоге, он равен $\poly(n)$
} 

\textbf{2. Разберем самое начало пункта VI PARAMETER SELECTION AND ASYMPTOTIC ANALYSIS до подпункта A.}

После каждого шага, позиции, который уже были обработаны, должны оставаться на некоторой заданной магнитуде $B$. Т.е. среднее (абсолютное значение) обработанной позиции должно быть очень близким к $B$. Это обусловлено тем, что мы применяем просеивание на каждом шаге редукции. После $t$ шагов мы получаем векторы средней нормы $\sqrt{n}\cdot B$.\footnote{E: именно так} 

Зададим число выборок равным ${m=2^k}$, где $2^k$ это параметр, который будет определять общую сложность алгоритма \textcolor{red}{(можете немного пояснить?)}\footnote{E: в этом применяется следующий подход: 1. сложность алгоритма, очевидно, зависит от количества необходимый выборок. Более того, если для верной работы алгоритмы нам нужно $m$ выборок, то время работы всего алгоритма будет напрямую зависеть от $m$. Конкретнее, время работы будет как минимум $\bigO(m)$, так как нам нужно все элементы выборки как минимум записать в список. 2. Мы ``знаем'', что размер необходимой выборки  будет экспоненциальным от $n$, т.е.\ $m = 2^{c \cdot n + \smallo(n) }$, где $c$ - некая константа (``знаем'' мы с учетом анализа предыдущих алгоритмов BKW.) Вопрос в том, чему же равна эта константа (в обозначения статьи $k=c_0 \cdot n$). Если она будет меньше, чем для предыдущих алгоритмов, то скорее всего, новый алгоритм работает быстрее. Что и происходит в нашем случае.}
мы получим примерно $m=2^k$ выборок после $t$ шагов. Как было описано ранее в статье, полученные выборки будут примерно Гауссовыми с дисперсией $\sigma^2\cdot(nB^2+2^t)$. Мы предполагаем, что лучшая стратегия это сохранение магнитуд двух различных \textcolor{red}{contributions (не очень понял, как корректно перевести)}\footnote{E: здесь имеется ввиду двух слагаемых $nB^2$ и $2^t$. Т.е.\ оба слагаемых ``добавляют'' (contribute) одинаковое количество шума в финальную выборку} одного и того же порядка, т.о. мы выбираем $nB^2 \approx 2^t$.

Далее, используя равенство $C \cdot e^{2\pi}(\frac{\sigma \sqrt{2\pi}}{q})^2$, \footnote{E: поясните, что такое $C$, откуда взялось это выражение, и что оно обозначает} чтобы иметь возможность восстановить одну секретную позицию, используя m выборок, нам необходимо $m=\bigO\left(e^{4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2}}\right)$.\footnote{E: вам понятно, откуда взялось это необходимое условие? Если нет, обратитесь к секции 4.2 Hypothesis testing в \url{https://eprint.iacr.org/2015/056.pdf}. Еще я про это рассказывала на прошлой неделе, в моем черновике доклада на первой странице я говорю про bias (``уклон'') полученного в результате Гауссового распределения }

Т.о., мы имеем $\ln{2} \cdot k = 4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2} + \bigO(1)$.

\textcolor{red}{Как уже сказал ранее, я вообще не разбираюсь в символах O и остальных. Можете в качестве примера и объяснения разобрать этот и предыдущий ($m$ и $\ln{2} \cdot k$) результат?}\footnote{E: здесь примерами не обойтись. Надо прочесть \url{http://web.mit.edu/16.070/www/lecture/big_o.pdf}
Для понимания анализа любого алгоритма, даже сортировки пузырьком, символы Ландау необходимы
}
\footnote{Второе выражение получено из первого взятием натурального логарифма и а также замечанием, что $\bigO(\exp(n)) = c \cdot \exp(X) + \smallo(\exp(X))$ для некой константы $c$, следовательно, $ \ln(\bigO(\exp(n)) ) = \ln(c) + X = \bigO(1) + X$ ($\bigO(1)$ -- обозначение константы $\ln(c)$).  }

Каждый из $t$ шагов должен доставить $m=2^k$ векторов вида описанного ранее.\footnote{E: Это не совсем верно, хоть так и написано в статье. На каждом шаге мы теряем $\bigO(2^k)$ векторов. Но так как шагов у нас будет всего $t = \bigO(\log n)$ (более конкретное значение дано в уравнении $(8)$ статьи), мы можем считать, что на выходе мы будем иметь $2^k$ векторов, при этом увеличив количество векторов на входе до $\approx \log n \cdot 2^k$. На асимптотику это фактор $\log(n)$ не влияет}

Поскольку мы имеем две части на каждом шаге редукции, нам необходимо проанализировать эти части по отдельности. Во-первых, рассмотрим выполнение первой части $i$ - го шага редукции, используя Coded-BKW с $[n_i,d_i]$ линейным кодом, где параметры $n_i$ и $d_i$ на каждом шаге выбраны для оптимального (глобально) выполнения. Мы сортируем $2^k$ векторов по $K=\frac{q^{d_i - 1}}{2}$ различным спискам. Здесь Coded-BKW шаг гарантирует, что все векторы в списке, ограниченные $n_i$ рассматриваемыми позициями, имеют среднюю норму не более чем $\sqrt{n_i}\cdot B$, если кодовое слово вычитается из вектора. Т.о. количество списков $\frac{q^{d_i - 1}}{2}$ должно быть выбрано так, чтобы это ограничение нормы выполнялось.\footnote{E: не совсем понимаю смысл этого предложения (в англ.\ версии тоже). Поясните} Затем, после шага Coded-BKW, шаг просеивания должен оставить среднюю норму по $N_i$ позициям без изменений, т.е. не более $\sqrt{N_i}\cdot B$.

Поскольку все векторы в списке, можно считать, имеют норму $\sqrt{N_i}\cdot B$ в этих $N_i$ позициях, шагу просеивания необходимо найти любую пару, которая оставляет разницу между нормами двух векторов не более $\sqrt{N_i}\cdot B$. Используя эвристику, что векторы равномерно распределены на сфере радиуса $\sqrt{N_i}\cdot B$, мы знаем, что один список должен содержать не менее $2^{0.208N_i }$ векторов, чтобы иметь возможность производить такое же количество векторов. Оценка времени и пространства\footnote{E: это только касается времени. Память ограничена $2^{0.208n }$. Авторы - не эксперты в LSH} равна $2^{0.292N_i}$, если используется LSF.

Примем некоторые дальнейшие обозначения. Поскольку мы ожидаем, что число векторов будет экспоненциально, мы пишем $k=c_0n$ для некоторого $c_0$.\footnote{E: это как раз отвечает на ваш вопрос страницей выше} Также, мы принимаем\footnote{E: скорее ``полагаем''} $q=n^{c_q}$ и $\sigma=n^{c_s}$. Выбрав $nB^2\approx2^t$, из формулы $\ln{2} \cdot k = 4\pi^2 \cdot \frac{\sigma^2 \cdot (nB^2+2^t)}{q^2} + O(1)$ выводим, что \textcolor{red}{$B=\Theta(n^{c_q-c_s})$} и \textcolor{red}{$t=(2(c_q-c_s)+1)\log_2 n + O(1)$}. \footnote{E: выражение для $t$ получено из выражения для $B$ и $nB^2 = 2^t$, думаю, это понятно. А выражение для $B$ получено из равенства выше, игнорируя константы (они спрятаны в обозначении $\Theta$)}

\end{document}