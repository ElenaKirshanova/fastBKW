In his seminal paper \cite{STOC:Regev05}, Regev shows that the Learning with Errors problem (see Def.~\ref{def:LWE}) is at least as hard as certain hard lattice problems. It is thus important to understand how hard these lattices problems really are. This chapter addresses this question.

In more detail, we treat the Learning with Errors Problem as a Bounded Distance Decoding (\BDD) Problem on the lattice $\qLat(A\transpose)$: given $(\AMat, \tvec = \AMat\transpose \svec + \evec \bmod q) \in \Z_q^{n \times m} \times \Z_q^m$, we want to find the nearest to $\tvec$ lattice point $\AMat\transpose \svec \in \qLat(A\transpose)$. In other words, we want to solve a decoding problem for a code generated by $\AMat\transpose$ with messages from $\Z_q^n$ and codewords from $\Z_q^m$. The solution $\AMat\transpose \svec$ is unique since $\norm{\evec} \ll \lambda_1(\qLat(\AMat\transpose))$.  The error-vector $\evec$ is sampled from the discrete Gaussian distribution with parameter $\alpha q$ over the integer lattice $\Z_q^m$.

We start with the \emph{asymptotical} hardness of lattice-based decoding attacks on \LWE. In Sect.~\ref{sec:LWEasBDDAs}, we analyze the complexity of the problem under the following algorithms: Babai's $\NP$ Algorithm \cite{STACS:Babai85}, its extension due to Lindner and Peikert \cite{RSA:LinPei11}, and the Generalized Pruning Algorithm \GenPrun{}  -- a unification of the decoding strategies which allows us to analyze the existing lattice-based algorithms for \LWE/\BDD. The so-called Pruning algorithms of \cite{EC:GamNguReg10, RSA:LiuNgu13} appear as special cases of \GenPrun.

The main \LWE parameter that determines the complexity is the dimension of the secret -- $n$. All the aforementioned decoding algorithms are either super-exponential: $2^{\const n \log n + \smallo(n \log n)}$, or singe-exponential $2^{\const' n + \smallo(n)}$, where $\const, \const'$ are constants that depend on the other two \LWE parameters: $q, \alpha$. The goal of Sect.~\ref{sec:LWEasBDDAs} is to determine these constants $\const, \const'$. Note that from the modulus-dimension trade-off by Brakerski et al.\ \cite{STOC:BLPRS13}, stating that $\LWE$ preserves its hardness as long as the value $n\log q$ stays the same,  we can already explain why for all algorithms, the leading order constants $\const, \const'$ have a multiple of $\cq = \log q/ \log n$. 

The results of our analysis are summarized in Table~\ref{table:compareTable} in Sect.~\ref{subsec:Summary} where we list the running times of \emph{all} known attacks on \LWE together with constants in the exponents. These constants are made explicit as functions of the \LWE parameters $q, \alpha$. The table is translated into Fig.~\ref{fig:LWEPlots} to give a clear `winner' among all the known attacks for a concrete choice of \LWE parameters.

The second part is devoted to \emph{practical} hardness of \LWE. In Sect.~\ref{sec:LWEasBDDPr}, we present real running times of the Linear-Length Pruning attack on \LWE{} -- a pruning strategy that appears to perform best for the Learning with Errors problem.

The results of the first section are mainly based on \cite{DCC:HKM}. The cryptanalysis of real \LWE instances presented in Sect.~\ref{sec:LWEasBDDPr} is published in \cite{ACNS:KMW16}.

\section{Asymptotical Hardness of \LWE} \label{sec:LWEasBDDAs}
For a decoding algorithm $\ALG$, we will be interested in the quantity $\rho(\ALG) = \frac{T(\ALG)}{\Psucc(\ALG)}$, the trade-off between the running time and the success probability of $\ALG$ (in other words, we are interested in the expected time to decode successfully).  For an \LWE instance $(\AMat, \tvec = \AMat\transpose \svec + \evec \mod q)$, the decoding is successful if the returned lattice vector is indeed $\AMat\transpose \svec$ or, equivalently, the returned error is $\evec$. In the decoding algorithms we actually search for $\evec$.  It is easy to verify whether a given $\evec$ is correct or not, as the correct one is much shorter than an error-vector that leads to a wrong solution.

The algorithm we analyze here is a two-phase \BDD decoding algorithm: first, we preprocess a basis for $\qLat(\AMat\transpose)$ (given in Eq.~(\ref{eq:LWEBasis})) using $\beta$-\BKZ reduction to obtain a shorter basis. Denote this basis as $\BMat$. 
We do not explain here how the reduction works. 
All we need for the analysis is the running time of the reduction and the quality of its output. 
We use Eqs.~(\ref{eq:b1norm}) and (\ref{eq:GSA}) as guarantees on the quality of $\BMat$. 
Under these guarantees, during the second phase, we form a search space
to enumerate candidates for the error $\evec$ within this search space. The shortest candidate $\evec'$ is output as the solution.  We now explain how the actual decoding, i.e.\ enumeration, works.

Enumeration is done via orthogonal projection of the target vector $\tvec$ onto a (close) translate of the lattice $\Lat(\BMat)$: $i \bvec_m + \Span(\bvec_1, \ldots, \bvec_{m-1})$ for some appropriately chosen $i \in \Z$. A projected vector that now belongs to an $m-1$-dimensional $\Span(\bvec_1, \ldots, \bvec_{m-1})$ together with the lattice $\Lat(\bvec_1, \ldots, \bvec_{m-1})$ forms a new \BDD instance. We can run this procedure recursively. After $m$ such recursive projections, we end up with a lattice-vector (the last projection onto a zero-dimensional space is just choosing a close point) and hope it is the closest to the original $\tvec$.

The way we choose close translates on each recursive step defines the search space of the enumeration: restricting the search to the fundamental parallelepiped of the Gram-Schmidt basis of the lattice $\FP(\wBMat)$ gives Babai's $\NP$ algorithm \cite{STACS:Babai85}. 
Enlarging it by stretching $\FP(\wBMat)$ to a parallelepiped $ \FP(\wBMat \cdot \DMat)$ for some diagonal matrix $\DMat$ results in the Linder-Peikert $\NPs$ algorithm \cite{RSA:LinPei11}.
Considering error-vectors $\evec$ that lie within some ball of radius $R$ gives rise to the Spherical Pruning \cite{SchE94}.
The so-called Linear-Length pruning of \cite{EC:GamNguReg10} forms a search space of cylinder intersections (i.e.\ a candidate error-vector $\evec$ is enumerated if its coordinates satisfy the system of inequalities $\{e_1^2 \leq R_1, e_1^2 + e_2^2 \leq R_2, \ldots, \| \evec \|^2 \leq R_n \}$ for some input-sequence $R_1, \ldots, R_n$).

Certainly, one is free to choose a search space of any shape and hope it will bring an improvement to the enumeration phase. By `improvement' we mean a better running-time/success probability trade-off $\rho$. We notice that the aforementioned pruning strategies share some common `rules', which allows us to analyze them at one shot.  To do that we define what we call a \emph{reasonable} pruning strategy that describes these `rules', give the algorithm $\GenPrun$ that follows this strategy, and analyze its complexity. It turns out that the Spherical Pruning, Linear-Length Pruning and other (called `Extreme' in \cite{EC:GamNguReg10}) pruning strategies are all reasonable, so it is sufficient to consider our generalization to conclude on their asymptotics.

To achieve our original goal -- to determine the complexity of \LWE as a \BDD problem -- we show in Sects.~\ref{sec:BabaisNP} -- \ref{sec:GenPrun} the running time/success probability trade-off $\rho(\ENUM) = \frac{T(\ENUM)}{\Psucc(\ENUM)}$, where we consider $\ENUM \in\{ \text{Babai's } \NP, \allowbreak \text{Lindner-Peikert's } \NPs, \GenPrun \}$. Since $\ENUM$ is only the second step of the whole algorithm, the actual trade-off on the \BDD attack is $\rho(\BDD) = \frac{T(\BKZ) + T(\ENUM)}{\Psucc(\ENUM)}$. We take care about it in Sect.~\ref{sec:Balance}.

\input{Babai}

\input{LindnerPeikert}

\input{GenPruning}

\input{Balance}

\input{OtherAttacks}

%\clearpage

\input{Summary}

\clearpage
