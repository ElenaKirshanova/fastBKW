\subsection{Other lattice-based algorithms for \LWE} \label{sec:OtherAttacks}

To provide a complete picture on lattice-based attacks, we briefly describe two other algorithms one can use to solve \LWE. First is the so-called Kannan's homogenization technique \cite{Kan87}, which allows us to convert a $\CVP$ (resp.\ $\BDD$) instance to an $\SVP$ (resp.\ $\uSVP$) instance in a higher-dimensional lattice (the approximation parameter $\gamma$ in $\uSVP$ depends on the promise given in the original \BDD instance). This approach is know as the \emph{embedding} technique. We show the running time of this attack when applied to $\LWE$ in Thm.~\ref{thm:Embed}. An analogous result was presented in \cite{APS15} but our choice of $m$ is different.

The second approach we consider, the so-called \emph{dual} attack, tackles the \emph{decisional}-\LWE (see Def.~\ref{def:decLWE}) by solving $\appSVP$ (for an appropriate choice of $\gamma$) in the lattice dual to the \LWE lattice. 

\paragraph{Embedding.} $\mkern-6mu$ Assume a \CVP instance $(\Lat(\BMat), \tvec)$ has a solution $\vvec \in \Lat(\BMat)$. Consider a higher-dimensional lattice $\LEmb (\BMat, \tvec)$ generated by the columns of the matrix
\begin{equation} \label{eq:BEmbed}
	\BMat' = \begin{pmatrix}
				\BMat & \tvec \\
				0 & \tau
			\end{pmatrix},
\end{equation}
 where $\tau$, known as the \emph{embedding factor}, is chosen such that the shortest vector in $\LEmb(\BMat, \tvec)$ is of the form $(\vvec, e)$ for $e \neq 0$. In other words, solving $\SVP$ on $\LEmb(\BMat, \tvec)$ leads to a solution of the original $\CVP$ problem. Note that $\tau$ should not be too small (in the worst-case $\CVP$ instance, $\tau$ is $\tfrac{1}{2} \lambda_1(\Lat(\BMat))$), otherwise the last column of the matrix $\BMat'$ may be used too often and the returned shortest vector may be the closest to a multiple of $\tvec$. On the other hand, $\tau$ should not be too large, otherwise the returned shortest vector will lie in $\Span(\BMat)$ giving no information on the solution to \CVP. 
 
 The embedding technique becomes more powerful if we know some information on the $\CVP$ instance we are given. In the $\BDD$ problem, for instance, we have a promise that the target vector is much closer to the solution-vector $\vvec$ than to any other lattice-vector. In this case, Kannan's technique leads us to the $\uSVP$ problem. In case of \LWE, we know that $\| \tvec - \vvec \| = \| \evec \| = \TLandau(\alpha q \sqrt{m})$, and, further, we know $\lambda_1(\BMat)$. It allows us to estimate $\lambda_1$ and $\lambda_2$ in $\LEmb(\BMat, \tvec)$ as $\lambda_1^2= \| \evec \|^2 + \tau^2$, $\lambda_2 = \lambda_1(\BMat)$, and hence, we know the gap of the lattice $\LEmb$. This gives us a bound on parameter $\gamma = \frac{\lambda_2}{\lambda_1}$ in the $\uSVP$ problem we are solving. 

We solve the $\uSVP$ problem via lattice-basis reduction. From Eq.~(\ref{eq:b1norm}), we know that an $m$-dimensional $\beta$-\BKZ reduced lattice, gives a $\beta^{\frac{m}{2 \beta}}$ approximation to a shortest vector of the lattice. Hence, as soon as the first (the shortest) vector of the reduced basis satisfies $\| \bvec_1 \| < \lambda_2(\LEmb(\BMat, \tvec))$, this vector is the shortest in $\LEmb(\BMat, \tvec)$ and, consequently, is the solution to $\uSVP$. All that remains is to determine the \BKZ parameter $\beta$ for which the first vector meets the requirement. 

In the theorem below, we consider the two possible running times of $\BKZ$, super-exponential (resp.\ single-exponential) as $f(\beta) = \beta \log \beta + \smallo(\beta \log \beta)$ (resp.\ $f(\beta) = \beta + \smallo(\beta)$). We omit the $\smallo(\cdot)$-terms. The space complexity of the attack is polynomial in the first case and exponential in $\beta$ in the second.
\begin{thm}[Complexity of the Embedding Attack on \LWE] \label{thm:Embed}
	The \LWE problem with parameters $(n, q=\bigO(n^{\cq}), \alpha = \bigO(1 / n^{\ca}) )$ where $\cq, \ca = \TLandau(1)$, can be solved via \emph{embedding} using a $\beta$-\BKZ reduction with $T(\BKZ) = 2^{\cBKZ \cdot f(\beta)}$, where either $f(\beta) = \beta$, or $f(\beta) = \beta \log \beta$ using the optimal choice of $m = \left( \frac{2 \cq}{\ca} + \smallo(1) \right) \cdot n$ of \LWE samples in time
	\[
		T(\Embed) = 2^{\left( \cBKZ \cdot \frac{2\cq}{\ca^2} + \smallo(1) \right) f(n)}.
	\]
\end{thm}

\begin{proof}
	Let $\BMat$ be a matrix that arises from $m$ \LWE samples and let $\LEmb(B, \tvec)$ be the corresponding $m+1$-dimensional lattice generated by the matrix defined in Eq.~(\ref{eq:BEmbed}). Let us estimate the first two successive minima $\lambda_1$, $\lambda_2$ for the embedded lattice. Setting the embedding factor $\tau = \TLandau(\| \evec \|)$, we obtain
	\[
		\lambda_1^2(\LEmb(\BMat, \tvec)) = \| \evec \|^2 + \tau^2 = \TLandau((\alpha q)^2 m).
	\]
	For a $q$-ary \LWE lattice, we have $\lambda_1(\Lat(\BMat)) =\min \{ q, \sqrt{m} q^{1 - n/m} \}$. For our choice of $m$, Minkowski's bound is always smaller, leading to
	\[
		\lambda_2(\LEmb(\BMat, \tvec)) = \lambda_1(\Lat(\BMat)) \leq \sqrt{m} q^{1 - n/m}.
	\] 
	The value of $\beta$, for which the $\beta$-reduced basis achieves an approximation of $\frac{\lambda_2(\LEmb(\BMat, \tvec))} { \lambda_1(\LEmb(\BMat, \tvec))}$, is given by
	\[
		\beta^{m / (2 \beta)} = \frac{\lambda_2(\LEmb(\BMat, \tvec))}{\lambda_1(\LEmb(\BMat, \tvec))} = \TLandau \left( \frac{q^{1-n/m}}{\alpha q} \right),
	\]
	assuming Minkowski's bound holds with equality. This is equivalent to $\beta = \left( \frac{2 \cq}{\ca^2} + \smallo(1) \right) \cdot n$. The minimum value for $\beta$ is attained at $m = \left( \frac{2 \cq}{\ca} + \smallo(1) \right) \cdot n$. 
\end{proof}

There is no surprise that the complexity of the embedding attack is exactly the same as the complexity of the two-phase \BDD attack with the single-exponential reduction followed by a polynomial-time enumeration (cf.\ Thm.~\ref{thm:BalanceSingleExp}). Both methods are, in fact, equivalent: performing a Babai-type (i.e.\ polynomial) enumeration on a reduced basis can be interpreted as embedding the target into the reduced basis and then size-reducing it (i.e.\ running \LLL). After such a procedure, the first vector reveals the closest vector.

\paragraph{Dual attack \hspace*{-8pt}}, originally considered in \cite{MicReg09} and further discussed in \cite{C:KirFou15}, solves the \emph{decisional}-\LWE. 

Given an \LWE instance $(\AMat, \tvec = \AMat\transpose \svec + \evec \bmod q) \in \Z_q^{n \times m} \times \Z_q^m$, instead of working with the lattice $\qLat(\AMat\transpose)$ (i.e.\ the image of $\AMat\transpose$) as we did so far, we make use of another $q$-ary $m$-dimensional lattice -- the kernel of $\AMat$:
\begin{align} \label{eq:DualLattice}
	\qLATTp (\AMat) = \{ \xvec \in \Z^m \colon \AMat \xvec = \zerovec \bmod q \}.
\end{align}
This lattice is the scaled dual to $\qLat(\AMat\transpose)$ with determinant $\det \qLATTp (\AMat) = q^n$ (the duality follows from the fact that $\qLATTp (\AMat) = q (\Lat(\AMat\transpose))^*$). A basis for this lattice can be easily formed from a (non-zero) matrix $\XMat$ that satisfies $\AMat \XMat = 0 \mod q$.

Assume we have found a short non-zero vector $\vvec \in \qLATTp (\AMat)$. Computing $w = \langle\vvec,\tvec\rangle \bmod q = \vvec^t (\AMat\transpose \svec + \evec ) \mod q = \ScProd{\vvec}{\evec} \bmod q$, we can distinguish whether $\evec$ is uniform or Gaussian. 
If $\evec$ is uniform, $w$ is also uniform, while if $\evec$ is Gaussian, $w = \sum_i v_i e_i$ is Gaussian with parameter $\alpha q \cdot \| \vvec \|$ (again, we assume the \LWE error follows a continuous Gaussian distribution). 
In the second case, the statistical distance between $w \bmod q$ and a uniform random variable $\bmod \ q$ or, a \emph{bias} of a continuous Gaussian with parameter $\alpha q \cdot \| \vvec \|$, is $\delta = 2^{-\bigO(\alpha^2 \| \vvec \|^2)}$. 
(To see this, we take a Fourier transform over $\Z_q$ of the Gaussian density function with standard deviation $\alpha q \| \vvec \|$ and evaluate at $1/q$). 
There exists an efficient distinguisher that has the advantage $\delta$ in deciding whether $w$ is uniform or Gaussian. 
For instance, [\cite{EC:DucTraVau15}, Lemma 10] shows that for Gaussian $w$ with standard deviation $s$ over $\Z_q$, $\E[\cos \bigl( \tfrac{2 \pi w}{q}\bigr)] \geq \frac{q}{\pi} \sin \left( \tfrac{\pi}{q}\right) e^{-2 \pi^2 s^2/q^2} $ (cf.\ line \ref{algline:decLWEDistinguish} in Alg.~\ref{alg:Dual}), while for a uniform $w \in \Z_q$, $\E[\cos \bigl( \tfrac{2 \pi w}{q}\bigr)] = 0$.

%
% DUAL algorithm
%
\setlength{\intextsep}{\medskipamount}
\begin{algorithm}[t]
\caption{ Dual attack on decisional-\LWE $\DUAL (\AMat, \protect \tvec, \eps)$}
\label{alg:Dual}
\textbf{Input:} $\AMat \in \Z^{m \times k}, \tvec \in \Z^m$ where  either (1) $\tvec = \AMat\transpose \svec + \evec \bmod q$ or (2) $\tvec$ is uniform from $\Z_q^m$  \\
\textbf{Output:} ``Yes'' if $\tvec = \AMat\transpose \svec + \evec \bmod q$, ``No'' otherwise.
\begin{algorithmic}[1]
	\State Compute $\XMat$ s.t.\ $\AMat \XMat = 0 \bmod q$ via Gaussian elimination
	\State $\BMat \gets$ $\beta$-\BKZ ($\qLat(\XMat)$) with $\beta = \left( \frac{2 \cq}{(1/2 + \ca)^2} + \smallo(1) \right) \cdot n$\Comment{
	\scriptsize
	Although we do not know $\ca$, we can approximate it by  \hspace*{9.1cm} trying several $\ca \in [0, \cq]$ in a binary-search manner} 
	\normalsize
	\If {$ \exists \vvec \in \Lat(\BMat)$ s.t.\ $\|\vvec \| =  \bigO(n^{1/2+\ca - \eps})$} 
		\normalsize
		\If {$\cos \left( \frac{ 2 \pi \langle{\vvec},{\tvec} \mkern3mu\rangle} {q} \right)> \frac{q}{\pi} \sin \left( \frac{\pi}{q}\right) e^{-2 \pi^2  \cdot n^{1-\eps}}$} \label{algline:decLWEDistinguish}
			\State \Return ``Yes''
		\Else
			\State \Return ``No''
		\EndIf
	\EndIf
\end{algorithmic} 
\end{algorithm}

To keep the bias $\delta = 2^{-\bigO(\alpha^2 \| \vvec \|^2)}$ sub-exponential, we must have $\| \vvec \| = \bigO(n^{1/2+\ca - \eps})$ for any $\eps>0$. The following lemma estimates the value for $\beta$ s.t.\ a $\beta$-\BKZ reduction (now on $\qLATTp (\AMat)$) outputs $\vvec$ of desired length. 

\begin{lemma}[Decisional \LWE under the $\DUAL$ attack] \label{lem:DecisionalLWE}
The \emph{decisional}-\LWE problem with parameters $(n,$ $q=\bigO(n^{\cq})$, $\alpha = \bigO(1 / n^{\ca}) )$ where $\cq, \ca = \TLandau(1)$, can be solved via running a $\beta$-\BKZ reduction on the \emph{dual} lattice defined as in Eq.~(\ref{eq:DualLattice}) with $T(\BKZ) = 2^{\cBKZ \cdot f(\beta)}$, where either $f(\beta) = \beta$, or $f(\beta) = \beta \log \beta$, using the optimal choice of $m = \left( \frac{2 \cq}{1/2 + \ca} + \smallo(1) \right) \cdot n$ of samples in time
	\[
		T(\DUAL) = 2^{\left( \cBKZ \cdot \frac{2\cq}{(1/2+\ca)^2} + \smallo(1) \right) f(n)}.
	\]
with $\Psucc(\DUAL) = 2^{-\bigO(n^{1-\eps})}$ for $\eps>0$.
\end{lemma}

\begin{proof}
	From Eq.~(\ref{eq:b1norm}), the shortest vector of a $\beta$-\BKZ reduced basis of $\qLATTp (\AMat)$ satisfies $\| \vvec \| =\bigO \Bigl(\beta^{\tfrac{m}{2 \beta}} q^{\tfrac{n}{m}}\Bigr)$. Since we want the bias $\delta = 2^{-\bigO(\alpha^2 \| \vvec \|^2)}$ remain sub-exponential, the length of $\vvec$ should additionally satisfy $\|\vvec \|= \bigO(n^{1/2+\ca - \eps})$. Letting $\eps \rightarrow 0$, we choose $\beta = \left( \frac{2 \cq}{(1/2 + \ca)^2} + \smallo(1) \right) \cdot n$ and $m = \left( \frac{2 \cq}{1/2 + \ca} + \smallo(1) \right) \cdot n$. The theorem follows after substituting this $\beta$ into the running time of $\BKZ$.
\end{proof}

One of the remarkable properties of \LWE is the equivalence between the \emph{decisional} and \emph{search} versions of the problem. While the direction decisional-\LWE $\leq$ search-\LWE is trivial, the reverse is not that immediate. But it was proved to be true in several papers starting with the original result of Regev \cite{STOC:Regev05} for prime $q = \poly(n)$ and later extended to exponentially large composite moduli \cite{EC:MicPei12}. 

Now assume we want to use this equivalence to turn the result of Lemma \ref{lem:DecisionalLWE} into an algorithm for the search-\LWE. Note that the search-to-decision reduction requires a decisional-\LWE oracle that returns the correct answer (i.e.\ given a pair ($\avec, t$), it decides whether it is uniform or follows an \LWE distribution) with success probability $1-\smallo(1)$. The advantage of the distinguisher from Alg.~\ref{alg:Dual} is only sub-exponential: $\delta = 2^{-\bigO(n^{1-\eps})}$. In order to boost the advantage to $1-\smallo(1)$, we have to repeat the algorithm $\poly(n) \delta^{-2}$ times on independent \LWE samples.\footnote{More precisely, if we have found $m = \poly(n) \delta^{-1}$ short enough vectors $\vvec_i \in \qLATTp (\AMat_i)$, in Alg.~\ref{alg:Dual} we rather compute $\tfrac{1}{m}\sum_i \cos \left( \frac{ 2 \pi \langle{\vvec_i},{\tvec_i}\rangle} {q} \right) $ and check if the result is large enough. The correctness follows from Chernoff bounds. Note that for uniform $\tvec_i$'s, the expected value of this sum is 0.} 

In case we have sub-exponentially many \LWE samples, the asymptotical complexity stated in Lemma~\ref{alg:Dual} also holds for the search-\LWE, as the additional sub-exponential term that comes from the search-to-decisional reduction is suppressed by the leading-order single/super-exponential term. 

In a more natural scenario, when the number of \LWE samples is limited to only $\poly(n)$, we can resort to the so-called amplification technique aimed at creating exponentially many `fresh looking' \LWE samples out of $\poly(n)$-many samples. This amplification -- originally considered for the combinatorial \BKW attack on \LWE \cite{DCC:ACFFP15} -- can also be applied to the $\DUAL$ attack to generate new samples. 

We now briefly describe how to generate many new samples and refer the reader for the complete proof to \cite{DCC:HKM}. Given an \LWE instance $(\AMat, \tvec) \in \Z_q^{n \times m} \times \Z_q^m$ with $m = \poly(n)$, we sample a discrete Gaussian $\xvec \in \Z_q^m$ with parameter $\eta = \WLandau(1)$ and output $(\AMat \xvec \bmod q, \ScProd{\tvec}{\xvec} \bmod q) \in \Z_q^n \times \Z_q$. This tuple serves as a new \LWE sample. 

 
The main challenge in the amplification process is to show that (1) the new $\avec' = \AMat \xvec \bmod q$ is uniformly distributed over $\Z_q^n$ and independent from the original samples, and (2) the new error $e'$ in $\ScProd{\tvec}{\xvec} \bmod q$ is independent from $a'$ conditioned on the original samples. For a wide enough Gaussian $\xvec$ (i.e.\ taking $\eta$ to be a large constant in case $m = \TLandau(n \log n)$, or even $\eta = \TLandau(n^{c_{\eta}})$ for small constant $c_{\eta}$ in case we have only $m = \TLandau(n)$ original samples), the amplification by $\xvec$ was shown to satisfy the above conditions, \cite{DCC:HKM}. 

Note that the width of the error in this new amplified \LWE samples gets increased from $\alpha q$ to $\sqrt{m} \alpha q$, thus adding $1/2$ to $\ca$ (in case $\eta = \TLandau(1)$). Combining amplification with the result obtained for the $\DUAL$ attack on decisional-\LWE in Lemma~\ref{lem:DecisionalLWE}, yields the following theorem. 

\begin{thm}[Search-\LWE under the $\DUAL$ attack] \label{thm:DualSearch} The \emph{search}-\LWE problem with parameters ($n$, $q=\bigO(n^{\cq})$, $\alpha = \bigO(1 / n^{\ca})$) where $\cq, \ca = \TLandau(1)$ and $m$ \LWE samples, can be solved via running $\beta$-\BKZ reduction on the \emph{dual} lattice defined in Eq.~(\ref{eq:DualLattice}) with $T(\BKZ) = 2^{\cBKZ \cdot f(\beta)}$, where either $f(\beta) = \beta$, or $f(\beta) = \beta \log \beta$, in time
\[
		T(\DUAL) = \begin{cases}
			2^{\left( \cBKZ \cdot \frac{2\cq}{(1/2+\ca)^2} + \smallo(1) \right) \cdot f(n)} \quad &  \text{if} \;\; m = 2^{\bigO(n)} \\
			2^{\left( \cBKZ \cdot \frac{2\cq}{\ca^2} + \smallo(1) \right) \cdot f(n)} \quad & \text{if} \;\; m = \WLandau(n \log n).
		\end{cases}
\]
with $\Psucc(\DUAL) = 1- \smallo(1)$.
\end{thm}

The memory complexity of the attack is determined by the $\beta$-\BKZ reduction. Note that we do not have to store all the $2^{\bigO(n)}$ \LWE samples required by the decision-to-search reduction, but rather access (or amplify) them once needed. 