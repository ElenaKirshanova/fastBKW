\subsection{Details on Implementation} \label{sec:DetailsOnImplementation}

We implemented our \BDD{} enumeration step choosing Linear Length Pruning as a bounding strategy.
All programs are written in \textsc{C++} and we used \textsc{C++11 STL} for implementing the threading. Our tests were performed on the Ruhr University's
{Crypto Crunching Cluster}~(C3) \cite{C3} which consists of one master node to schedule jobs and four computing
nodes. Each computing node has four AMD Bulldozer Opteron 6276 CPUs, and thus 64~cores,
running at 2.3 GHz and 256 GByte of RAM\@. The results of our experiments are
presented in Table~\ref{tabel:RunTimesLWE}. Let us take a closer look at this table.

All instances are split into tree categories depending on the noise-rate: the left-most have $\alpha=0.001$, middle $\alpha=0.002$, right-most $\alpha=0.005$. The instances for the first two cases were generated by ourselves with modulus $q=4093$, while for the last case, we attack the instances offered by the \LWE-Challenge \cite{LWEChallenge}. For $n=40, 45, 50$, the moduli are $q=1601, 2027, 2053$ respectively.

For the \LWE instances with small error-rate, we took $m=2n$ samples. For $\alpha=0.002$, to aid the enumeration step, we  slightly increase the number of samples to $m \approx 2.2n$. Notice, that for a larger $m$, the determinant of the \LWE-lattice, $\det(\qLat(A\transpose)) = q^{1-n/m}$, increases. This leads to a larger $\lambda_1(\qLat(A\transpose))$, making the error, from the enumeration point of view, closer to the target. Larger $m$ explains why the running-time for $\BKZ$ with the same block-size $\beta$ increases for the large noise-rates. For $\alpha=0.005$, we use even more samples $m \approx 2.3n$. Note that theory suggests an increased $m$ as well: in Thm.~\ref{thm:BalanceSuperExp}, the optimal choice of $m$ was proved to be $m=\left( \frac{2 \cq}{\sqrt{2 \cBKZ}+\ca} + \smallo(1) \right) \cdot n$. Recall that larger noise-rate corresponds to smaller $\ca$. 

For the pruning strategy $\B$, we choose the \emph{Linear-Length} pruning function \cite{EC:GamNguReg10}. This means that our tree-traversal Algorithm \ref{alg:GenPrunDepth} receives on input an $m$-dimensional array $R$ consisting of level-bounds $R_{m-k} = \cf ((m-k) (\alpha q)^2)$, where $k$ goes from $m$ down to $0$. These bounds determine the allowed accumulated error-length per level. $\cf$ is an additional input-constant. The larger $\cf$ we input, the bushier the enumeration tree is and, hence, the more expensive the algorithm is.

Since we know the length of the Gram-Schmidt basis-vectors $\wbvec$, we can determine the maximal level $k$ (called `critical' from our asymptotical analysis in Sect.~\ref{sec:LWEasBDDAs}), for which $\|\wbvec_k \| > \const \alpha q$ for a constant $\const$ which we set $\const = 2$. From this level down, we run Babai's Algorithm~\ref{alg:Babai}.

From the experiments, we draw the following conclusions. 
	
\vspace{8pt} \hspace{5pt} \textbf{Enumeration can be perfectly parallelized.} Indeed, the way we schedule the jobs for the parallel tree-traversal in Algorithm~\ref{alg:Breadth_first} allows for the speed-up equal to the number of available threads. Recall that in Algorithm~\ref{alg:Breadth_first}, we create much more sub-trees (i.e.\ jobs) than the number of threads $\nT$, and store the roots of these sub-trees in a queue. An additional input parameter $\const$ determines the size of this queue. In our tests, we set this parameter large enough to guarantee that the number of jobs in the queue is of order $5000-6000$ (for $\nT = 10$, it corresponds to $\const=500-600$). For the dimensions we tackle, these numbers are larger enough to guarantee that there will many equally big sub-trees and all the threads will be evenly occupied. It is reasonable to predict that for higher dimensions and/or more threads at hand, one should choose queues of larger size. 

An almost perfect speed-up was achieved for all dimensions where we run the parallelized enumeration: for $\alpha=0.001$ and dimensions $n=70,80,90$, our multi-threaded implementation allows to choose relatively small $\beta$'s for the reduction and hence, to balance out the running times for the reduction and enumeration. Based on the experiments for these dimensions, for some other instances only the parallelized version was run. For example, for $n=75$, both reduction and enumeration on 10 threads were finished in about an hour. On instances with larger noise-rates, the tests were mostly run in the multi-threaded regime as enumeration becomes significantly slower.


\vspace{8pt} \hspace{5pt} \textbf{Binary error is significantly easier for enumeration.} We also performed some tests on instances with a binary noise (this version of \LWE also admits a hardness reduction, \cite{C:MicPei13}, but for a restricted number of samples $m = \bigO(n)$). For such a small error-rate, enumeration is fast, so in order to balance the attack, we choose a \emph{smaller} $m$ (but still large enough to guarantee the unique solution). This speeds up the reduction but slows down the enumeration. Again, we mitigate this slow-down with parallelization. In Table~\ref{table:RunTimesBinError} below, we choose $n=130$ as an example. Note that for this dimension, the attack runs in approximately the same time as for $n=75$ in the Gaussian-error case with small noise-rate.  

\vspace{10pt}
\begin{table}[h]
	\centering
	\begin{tabular}{ccc|cc|cc}
		\toprule
		\multicolumn{3}{c|}{\LWE-parameters}  & \multicolumn{2}{c|}{\BKZ-reduction}  & \multicolumn{2}{c}{Length Pruning} \\
		$n$   & $q$                      & $m$   & $\beta$ & $T$         & $\nT$ &  $T$      \\\midrule     
		$130$ & $4093$     & $190$ & $18$     & 1.6e4     & 1 & 4.8e4  \\ 
		$130$ & $4093$     & $190$ & $18$     & 1.6e4     & 10 & 6.1e3  \\
		\bottomrule
	\end{tabular}
	\caption{Running times of the \BDD-decoding attack on binary \LWE}
	\label{table:RunTimesBinError}
\end{table}

\vspace{8pt} \hspace{5pt} \textbf{An increase in the error-rate causes a substantial slow-down for the attack.} Indeed, in case of the large noise-rate of $\alpha=0.005$, the attack performs significantly worse than for a smaller rates. In order to obtain long enough Gram-Schmidt vectors for successful decoding, we have to (1) increase $m$, and (2) increase $\beta$. Both factors result in slower \BKZ weakening the whole attack. 

\vspace{8pt}

We would like to mention that a couple of months after the publication of \cite{ACNS:KMW16}, the \LWE Challenge was announced in \cite{LWEChallenge}. Currently, the attack that tackles the hardest parameter-sets is the parallelized two-phase decoding.