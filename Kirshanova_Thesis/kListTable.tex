\subsection{Experimental results} \label{subsec:KListResults}
We implement the $3$-Gauss sieve Algorithm~\ref{alg:3GaussSieve} in collaboration with S.\ Bai \cite{Bai16}. 
The implementation is based on the program developed by Bai, Laarhoven, and Stehl\'{e} in \cite{BLS16}. 
The experiments are run on the Ruhr University C3 cluster \cite{C3}. The results are presented in Table~\ref{table:kListExperiments}.

Lattice bases are generated by the \SVP challenge generator \cite{SVPChallenge}. It produces a lattice generated by the columns of the matrix
\begin{align*} B=
\begin{psmallmatrix}
	p & x_1 & \ldots & x_{n-1} \\
	0 & 1 & \ldots &  0  \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \ldots & 1 \\
\end{psmallmatrix},
\end{align*}
where $p$ is a large prime, and $x_i< p$ for all $i$. Lattices of this type are random in the sense of Goldstein and Mayer \cite{GoldMay06}.

For all the dimensions except $n=80$, the bases are preprocessed with \BKZ reduction of block-size~$20$. For $n=80$, the block-size is $30$. For our input lattices, we do not know their minimum $\lambda_1$. The algorithm terminates when it finds many linearly dependent triples $(\pvec, \vvec_1, \vvec_2)$. It means that at some point $\Call{TripleReduce}$ starts outputting $\zerovec$. We set a counter for such an event and terminate the algorithm once this counter goes over a pre-defined threshold. The intuition behind this idea is straightforward: at some point the list $L$ will contain very short basis-vectors and the remaining list-vectors will be their linear combinations. Trying to reduced the latter will ultimately produce the zero-vector. The same termination condition was already used in \cite{BLM15}, where the authors experimentally determine a threshold of such `zero-sum' triples.

Up to $n=64$, the experiments are repeated 5 times (i.e.\ on 5 random lattices), for the dimensions less than $80$, 3 times. For the running times and the list-sizes presented in the table below, the average is taken. For $n=80$, the experiment was performed once.  

Our tests confirm a noticeable speed-up of the 3-Gauss sieve when our Configuration Search Algorithm~\ref{alg:AlgConfig} is used. Moreover, as the analysis suggests (see Fig.~\ref{fig:RunTimes}), our algorithm outperforms the naive 2-Gauss sieve while using much less memory. 

Another interesting aspect of the algorithm is the list-sizes when compared with BLS. Despite the fact that asymptotically the size of the list $|L|$ is the same for our and for the BLS algorithms, in practice our algorithm requires a longer list (cf.\ the right numbers in each column). This is due to the fact that we filter out a larger fraction of solutions. Also notice that increasing $\eps$ -- the approximation to the target configuration -- we achieve an additional speed-up. This becomes obvious once we look at the $\Call{Filter}$ procedure: allowing for a smaller inner-product throws away less vectors, which in turn results in a shorter list $L$. For the range of dimensions we consider, the optimum is attained at $\eps=0.3$.

\renewcommand{\arraystretch}{1.9}
\begin{table}[t]
	\centering
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{|c|c|c|c|c|c|c|} \hline
		 & \multirow{2}{*}{$2$-sieve} & \multirow{2}{*}{BLS $3$-sieve} & \multicolumn{4}{c|}{ Alg.~\ref{alg:3GaussSieve}, $3$-sieve} \\ \cline{4-7}
		& & & $\eps = 0.0$ & $\eps = 0.015$ & $\eps = 0.3$ & $\eps = 0.4$ \\ \hline
		$n$ & $T$, $\normalabs{L}$ & $T$, $\normalabs{L}$ & $T$, $\normalabs{L}$ & $T$, $\normalabs{L}$ & $T$, $\normalabs{L}$ & $T$, $\normalabs{L}$ \\ \hline
		60 & 1.38e3, 13257 & 1.02e4, 4936& 1.32e3, 7763& 1.26e3, 7386& 1.26e3, 6751 & \textbf{1.08e3, 6296} \\ \hline
		62 & 2.88e3, 19193 & 1.62e4, 6239 & 2.8e3, 10356 & 3.1e3, 9386 & \textbf{1.8e3, 8583} & 2.2e3, 8436 \\ \hline
		64 & 8.64e3, 24178 & 5.5e4, 8369 & 5.7e3, 13573& 3.6e3, 12369 & \textbf{3.36e3, 11142}& 4.0e4, 10934\\ \hline
		66 & 1.75e4, 31707 & 9.66e4, 10853 & 1.5e4, 17810 & 1.38e4, 16039 & \textbf{9.1e3, 14822}& 1.2e4, 14428 \\ \hline
		68 & 3.95e4, 43160 & 2.3e5, 14270 & 2.34e4, 24135 & 2.0e4, 21327 & \textbf{1.68e4, 19640}& 1.86e4, 18355 \\ \hline
		70 & 6.4e4, 58083 & 6.2e5, 19484 & 6.21e4, 32168 & 3.48e5, 26954 & \textbf{3.3e4, 25307} & 3.42e4, 24420 \\ \hline
		72 & 2.67e5, 77984 & 1.2e6, 25034& 7.6e4, 40671 & 7.2e4, 37091 & \textbf{6.16e4, 34063} & 6.35e4, 34032 \\ \hline
		74 & 3.45e5, 106654 & \textemdash& 2.28e5, 54198& 2.08e5, 47951& \textbf{2.02e5, 43661}& 2.03e5, 40882\\ \hline
		76 & 4.67e5, 142397 & \textemdash& 3.58e5, 71431& 2.92e5, 64620& \textbf{2.42e5, 56587} & 2.53e5, 54848 \\ \hline
		78 & 9.3e5, 188905 & \textemdash & \textemdash & \textemdash& \textbf{4.6e5, 74610} & 4.8e5, 70494 \\ \hline
		80 & \textemdash & \textemdash &\textemdash & \textemdash& \textbf{9.47e5, 98169} & 9.9e5, 98094\\ \hline
	\end{tabular}
	}	
	\caption[Experimental results for $k$-tuple Gauss sieve]{Experimental results for $k$-tuple Gauss sieve. The running times $T$ are given in seconds, $\abs{L}$ is the maximal size of the list $L$. $\eps$ is the approximation parameter for the subroutine $\Call{Filter}$ of Alg.~\ref{alg:3GaussSieve}. The best running-time per dimension is type-set bold.}
	\label{table:kListExperiments}
\end{table}